{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "SW_hoyW-k9AE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "-XXxdU2DXwAo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bggcHoGRXwAs"
      },
      "outputs": [],
      "source": [
        "# File paths\n",
        "train_file = 'NER-TRAINING.jsonlines'\n",
        "validation_file = 'NER-VALIDATION.jsonlines'\n",
        "test_file = 'NER-TESTING.jsonlines'\n",
        "predicted_file = 'ner-testing-predictions.jsonlines'\n",
        "predicted_file_2 = 'ner-validation-predictions.jsonlines'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the datasets"
      ],
      "metadata": {
        "id": "9-yOvpYnlOze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "BWZlzeivXwAt"
      },
      "outputs": [],
      "source": [
        "# Helper function to read JSON Lines file\n",
        "def read_jsonl(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "# Helper function to write JSON Lines file\n",
        "def write_jsonl(file_path, data):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for entry in data:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "# Clean data by removing entries with only 'O' tags\n",
        "def clean_data(dataset):\n",
        "    return [entry for entry in dataset if any(tag != 'O' for tag in entry.get('ner_tags', []))]\n",
        "\n",
        "# Extract features for each token\n",
        "def extract_features(tokens):\n",
        "    features = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_features = {\n",
        "            'token': token,\n",
        "            'is_upper': token.isupper(),\n",
        "            'is_title': token.istitle(),\n",
        "            'is_digit': token.isdigit(),\n",
        "            'prefix-1': token[:1],\n",
        "            'prefix-2': token[:2],\n",
        "            'suffix-1': token[-1:],\n",
        "            'suffix-2': token[-2:],\n",
        "            'length': len(token),\n",
        "            'position': i,\n",
        "        }\n",
        "        features.append(token_features)\n",
        "    return features\n",
        "\n",
        "# Prepare training data\n",
        "def prepare_training_data(dataset):\n",
        "    X, y = [], []\n",
        "    for entry in dataset:\n",
        "        tokens = entry['tokens']\n",
        "        ner_tags = entry['ner_tags']\n",
        "        features = extract_features(tokens)\n",
        "        X.extend(features)\n",
        "        y.extend(ner_tags)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx68gI4-XwAu",
        "outputId": "8b9fc9e5-0951-4077-c1fa-c11836f542ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    B-Action       0.58      0.38      0.46       416\n",
            "    B-Entity       0.55      0.56      0.56       923\n",
            "  B-Modifier       0.52      0.36      0.43       280\n",
            "    I-Action       0.43      0.24      0.31       110\n",
            "    I-Entity       0.66      0.65      0.66      2907\n",
            "  I-Modifier       0.00      0.00      0.00        22\n",
            "           O       0.60      0.66      0.63      3046\n",
            "\n",
            "    accuracy                           0.61      7704\n",
            "   macro avg       0.48      0.41      0.43      7704\n",
            "weighted avg       0.61      0.61      0.61      7704\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "train_data = read_jsonl(train_file)\n",
        "validation_data = read_jsonl(validation_file)\n",
        "test_data = read_jsonl(test_file)\n",
        "\n",
        "# Clean training and validation data\n",
        "train_data = clean_data(train_data)\n",
        "validation_data = clean_data(validation_data)\n",
        "\n",
        "# Prepare features and labels\n",
        "X_train, y_train = prepare_training_data(train_data)\n",
        "X_valid, y_valid = prepare_training_data(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "FFhwfxMQlhKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=False)),\n",
        "    ('classifier', LogisticRegression(max_iter=10000))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred_valid = pipeline.predict(X_valid)\n",
        "print(classification_report(y_valid, y_pred_valid))"
      ],
      "metadata": {
        "id": "NZ5mXJ_1lX1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the predictions"
      ],
      "metadata": {
        "id": "O1usvvUTl8X_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8KAiavzXwAv",
        "outputId": "3d07606d-4465-49eb-a7d5-753f977b39a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to ner-validation-predictions.jsonlines\n"
          ]
        }
      ],
      "source": [
        "# Predict for data of choice\n",
        "def predict_for_data(dataset, pipeline):\n",
        "    predictions = []\n",
        "    for entry in dataset:\n",
        "        tokens = entry['tokens']\n",
        "        features = extract_features(tokens)\n",
        "        predicted_tags = pipeline.predict(features)\n",
        "        predictions.append({\n",
        "            'unique_id': entry['unique_id'],\n",
        "            'tokens': tokens,\n",
        "            'ner_tags': predicted_tags.tolist()\n",
        "        })\n",
        "    return predictions\n",
        "\n",
        "# PREDICTIONS ON TEST DATA\n",
        "# predictions = predict_for_data(test_data, pipeline)\n",
        "# write_jsonl(predicted_file, predictions) # write predictions to json file\n",
        "# print(f\"Predictions saved to {predicted_file}\")\n",
        "\n",
        "# PREDICTIONS ON VALIDATION DATA\n",
        "validation_data_2 = read_jsonl(validation_file)\n",
        "predictions = predict_for_data(validation_data_2, pipeline)\n",
        "write_jsonl(predicted_file_2, predictions) # write predictions to json file\n",
        "print(f\"Predictions saved to {predicted_file_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation scores"
      ],
      "metadata": {
        "id": "1YNI9g5AmBEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
        "from seqeval.scheme import IOB2\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def pretty_print_dict(d, indent):\n",
        "    res = \"\"\n",
        "    for k, v in d.items():\n",
        "        res += \"\\t\"*indent + str(k) + \"\\n\"\n",
        "        if isinstance(v, dict):\n",
        "            res += pretty_print_dict(v, indent+1)\n",
        "        else:\n",
        "            res += \"\\t\"*(indent+1) + str(v) + \"\\n\"\n",
        "    print(res)\n",
        "    return res\n",
        "\n",
        "def compute_seqeval_jsonl(references_jsonl, predictions_jsonl, ref_col='ner_tags', pred_col='pred_ner_tags'):\n",
        "    '''\n",
        "    Computes the seqeval scores between two datasets loaded from jsonl (list of dicts with same keys).\n",
        "    Sorts the datasets by 'unique_id' and verifies that the tokens match.\n",
        "    '''\n",
        "    # extract the tags and reverse the dict\n",
        "    ref_dict = {k:[e[k] for e in references_jsonl] for k in references_jsonl[0].keys()}\n",
        "    pred_dict = {k:[e[k] for e in predictions_jsonl] for k in predictions_jsonl[0].keys()}\n",
        "\n",
        "    # sort by unique_id\n",
        "    ref_idx = np.argsort(ref_dict['unique_id'])\n",
        "    pred_idx = np.argsort(pred_dict['unique_id'])\n",
        "    ref_ner_tags = np.array(ref_dict[ref_col], dtype=object)[ref_idx]\n",
        "    pred_ner_tags = np.array(pred_dict[pred_col], dtype=object)[pred_idx]\n",
        "    ref_tokens = np.array(ref_dict['tokens'], dtype=object)[ref_idx]\n",
        "    pred_tokens = np.array(pred_dict['tokens'], dtype=object)[pred_idx]\n",
        "\n",
        "    # check that tokens match\n",
        "    #assert((ref_tokens==pred_tokens).all())\n",
        "\n",
        "\n",
        "    # get report\n",
        "    report = classification_report(y_true=ref_ner_tags, y_pred=pred_ner_tags,\n",
        "                                   scheme=IOB2, output_dict=True,\n",
        "                                  )\n",
        "\n",
        "    # extract values we care about\n",
        "    report.pop(\"macro avg\")\n",
        "    report.pop(\"weighted avg\")\n",
        "    overall_score = report.pop(\"micro avg\")\n",
        "\n",
        "    seqeval_results = {\n",
        "        type_name: {\n",
        "            \"precision\": score[\"precision\"],\n",
        "            \"recall\": score[\"recall\"],\n",
        "            \"f1\": score[\"f1-score\"],\n",
        "            \"suport\": score[\"support\"],\n",
        "        }\n",
        "        for type_name, score in report.items()\n",
        "    }\n",
        "    seqeval_results[\"overall_precision\"] = overall_score[\"precision\"]\n",
        "    seqeval_results[\"overall_recall\"] = overall_score[\"recall\"]\n",
        "    seqeval_results[\"overall_f1\"] = overall_score[\"f1-score\"]\n",
        "    seqeval_results[\"overall_accuracy\"] = accuracy_score(y_true=ref_ner_tags, y_pred=pred_ner_tags)\n",
        "\n",
        "    return(seqeval_results)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Pour les étudiants : indiquer le chemin vers le fichier NER-VALIDATION\n",
        "    with open(\"NER-VALIDATION.jsonlines\", 'r') as f:\n",
        "        references_jsonl = [json.loads(l) for l in list(f)]\n",
        "\n",
        "    # Pour les étudiants : indiquer ici le chemin vers votre fichier de prédiction sur le jeu de validation\n",
        "    with open(predicted_file_2, 'r') as f:\n",
        "        pred_jsonl = [json.loads(l) for l in list(f)]\n",
        "\n",
        "\n",
        "    res = compute_seqeval_jsonl(references_jsonl, pred_jsonl, ref_col = 'ner_tags', pred_col='ner_tags')\n",
        "    pretty_print_dict(res, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb07S24oWeEz",
        "outputId": "59254bf2-ed0e-4b35-b312-2686f5fcc29c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tprecision\n",
            "\t\t0.21887287024901703\n",
            "\trecall\n",
            "\t\t0.4014423076923077\n",
            "\tf1\n",
            "\t\t0.28329092451229854\n",
            "\tsuport\n",
            "\t\t416\n",
            "\n",
            "\tprecision\n",
            "\t\t0.0668328400062315\n",
            "\trecall\n",
            "\t\t0.4647887323943662\n",
            "\tf1\n",
            "\t\t0.11686189049305366\n",
            "\tsuport\n",
            "\t\t923\n",
            "\n",
            "\tprecision\n",
            "\t\t0.19186046511627908\n",
            "\trecall\n",
            "\t\t0.3535714285714286\n",
            "\tf1\n",
            "\t\t0.24874371859296485\n",
            "\tsuport\n",
            "\t\t280\n",
            "\n",
            "Action\n",
            "\tprecision\n",
            "\t\t0.21887287024901703\n",
            "\trecall\n",
            "\t\t0.4014423076923077\n",
            "\tf1\n",
            "\t\t0.28329092451229854\n",
            "\tsuport\n",
            "\t\t416\n",
            "Entity\n",
            "\tprecision\n",
            "\t\t0.0668328400062315\n",
            "\trecall\n",
            "\t\t0.4647887323943662\n",
            "\tf1\n",
            "\t\t0.11686189049305366\n",
            "\tsuport\n",
            "\t\t923\n",
            "Modifier\n",
            "\tprecision\n",
            "\t\t0.19186046511627908\n",
            "\trecall\n",
            "\t\t0.3535714285714286\n",
            "\tf1\n",
            "\t\t0.24874371859296485\n",
            "\tsuport\n",
            "\t\t280\n",
            "overall_precision\n",
            "\t0.09028319043907508\n",
            "overall_recall\n",
            "\t0.42927733168622606\n",
            "overall_f1\n",
            "\t0.14918965332188472\n",
            "overall_accuracy\n",
            "\t0.54665984735227\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "K7oiXSGfXwAw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}